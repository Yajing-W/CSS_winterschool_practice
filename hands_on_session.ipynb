{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Session 1: Web Browsing Traces\n",
        "## Introduction\n",
        "The topic of this session is **Digital Traces and Human Behavior**. We will provide a dataset of individuals' web browsing traces combined with their sociodemographic information collected via surveys.\n",
        "\n",
        "In this session,we will analyze the data to examine the patterns of individuals' internet use and how they vary across different demographic groups.\n",
        "\n",
        "### Learning Objectives:\n",
        "* Extract patterns of internet use from browsing traces.\n",
        "* Examine how browsing patterns differ across demographic groups.\n",
        "\n",
        "---\n",
        "### Steps:\n",
        "1. **Data Cleaning & Setup**: Preparing the raw browsing logs.\n",
        "2. **Individual profile**: Profiling a single individual's \"Digital Profile\".\n",
        "3. **Group Project**:\n",
        "    * **Task 1**: Classify the 50 users based on browsing rhythms.\n",
        "    * **Task 2**: Connect behaviors to demographics (Gender, Age, Politics).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TMEnIlc-u0m0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Programming Toolkit\n",
        "To perform this analysis, we will use several specialized Python libraries. You can refer to the official documentation for detailed API information:\n",
        "\n",
        "| Library | Category | Purpose | Documentation |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Pandas** | Data Processing | Data manipulation and DataFrame operations | [Docs](https://pandas.pydata.org/docs/user_guide/index.html#user-guide) |\n",
        "| **NumPy** | Mathematics | Numerical computing and array operations | [Docs](https://numpy.org/doc/stable/user/index.html#user) |\n",
        "| **Matplotlib** | Visualization | Core library for static plots | [Docs](https://matplotlib.org/stable/contents.html) |\n",
        "| **Seaborn** | Visualization | Statistical data visualization | [Docs](https://seaborn.pydata.org/tutorial/introduction.html) |\n",
        "| **WordCloud** | NLP/Visualization | Generating tag clouds from text | [Docs](https://amueller.github.io/word_cloud/) |\n",
        "| **Circlify** | Visualization | Circle packing for bubble charts | [GitHub](https://github.com/elmotec/circlify) |\n",
        "| **Stop-Words** | NLP | Filtering common words in multiple languages | [PyPI](https://pypi.org/project/stop-words/) |\n",
        "| **Scipy.stats** | Statistics | Performing t-tests to compare groups. |[Docs](https://docs.scipy.org/doc/scipy/reference/stats.html)|"
      ],
      "metadata": {
        "id": "HGdGsdl1vdes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specialized libraries\n",
        "!pip install circlify wordcloud stop-words\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import circlify\n",
        "import re\n",
        "import os\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from stop_words import get_stop_words\n",
        "from urllib.parse import unquote\n",
        "from scipy import stats\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plotting configuration\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "o3RImZoLu2Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset Descriptions\n",
        "\n",
        "| Dataset | Granularity | Key Columns | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Browsing traces** | High-frequency | `pid`, `used_at`,`url`,`domain`, `duration`, `wave` | Raw web traces recording every URL visited. |\n",
        "| **Demographics** | Static | `pid`, `gender`, `age_group`, `politics` | Constant user characteristics. |\n",
        "| **Domain Map** | Metadata | `domain`, `category` | Mapping URLs to functional categories (e.g., Social, News). |\n",
        "\n",
        "For privacy and sensitivity reasons, we removed all URLs except search-engine queries, which were kept for the word cloud analysis.\n",
        "\n",
        "\n",
        "#### **Demographic Coding**\n",
        "* **Gender**: `1` = Male, `0` = Female.\n",
        "* **Politics**: `1` = Left, `0` = Right.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZG2njddTFV-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/Yajing-W/CSS_winterschool_practice/refs/heads/main/\"\n",
        "\n",
        "df = pd.read_parquet(BASE_URL + \"browsing_data.parquet\")\n",
        "demo_df = pd.read_csv(BASE_URL + \"demographics.csv\")\n",
        "category_df = pd.read_csv(BASE_URL + \"domain_category_map.csv\")\n",
        "print(\"Datasets loaded successfully.\")\n",
        "\n",
        "print(f\"Browsing traces: {df.shape}\")\n",
        "print(f\"Demographics: {demo_df.shape}\")\n",
        "print(f\"Domain_category_map: {category_df.shape}\")"
      ],
      "metadata": {
        "id": "8Q5R43oV2tE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Cleaning (Valid Signal Identification)\n",
        "\n",
        "Raw browsing logs often contain noise such as background ads, automatic refreshes, or idle sessions. Before profiling users, we want to ensure the data represents genuine human intent and active engagement.\n",
        "\n",
        "> * When converting time, use `pd.to_datetime(..., utc=True)` to handle potential timezone issues efficiently.\n",
        "> * **Filtering**: In real-world datasets, a \"zero duration\" is often a tracking error or a failed page load.\n",
        "\n",
        "#### Basic Cleaning & Anomaly Detection\n",
        "1. **Time Conversion**: Convert the `used_at` column to Pandas `datetime` objects.\n",
        "2. **Standard Filter**: Remove records where `duration` is 0 or missing.\n",
        "3. **Ghost Session Identification**: Identify \"Ghost Sessions\" where `duration` exceeds 4 hours (14,400s). These are typically caused by tabs left open overnight or background processes (e.g., music/gaming)."
      ],
      "metadata": {
        "id": "9v3gld3avGu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime and ensure UTC consistency\n",
        "df['used_at'] = pd.to_datetime(df['used_at'], utc=True)\n",
        "\n",
        "# Remove zero/null durations\n",
        "initial_count = len(df)\n",
        "df = df[df['duration'] > 0].dropna(subset=['duration'])\n",
        "print(f\"Standard Cleaning: Removed {initial_count - len(df)} invalid records.\")\n",
        "\n",
        "# Anomaly Detection (Ghost Sessions)\n",
        "# Define threshold: 4 hours = 14400 seconds\n",
        "GHOST_THRESHOLD = 14400\n",
        "df['is_ghost'] = df['duration'] > GHOST_THRESHOLD\n",
        "\n",
        "ghost_count = df['is_ghost'].sum()\n",
        "ghost_time_share = df[df['is_ghost']]['duration'].sum() / df['duration'].sum()\n",
        "\n",
        "print(f\"Ghost Detection: Found {ghost_count} sessions > 4 hours.\")\n",
        "\n",
        "# Create a cleaned dataset for behavior analysis\n",
        "df_final = df[df['is_ghost'] == False].copy()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot: Boxplot comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(data=df, y=df['duration'] / 60)\n",
        "plt.title(\"Before Cleaning\")\n",
        "plt.ylabel(\"Duration (Minutes)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df_final, y=df_final['duration'] / 60)\n",
        "plt.title(\"After Cleaning\")\n",
        "plt.ylabel(\"Duration (Minutes)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J4egLbpovBbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.2: Behavioral Profiling & Visualization (EDA)\n",
        "\n",
        "With the noise removed, we can now answer: **\"How actively do our users engage online?\"** We will use three classic visualization methods to understand the population.\n",
        "\n",
        "#### Quantifying User Engagement\n",
        "1. **Active Days**: How many unique days did each user (`pid`) appear during the month?\n",
        "2. **Daily Usage**: On the days they were active, how many hours did they spend browsing on average?\n",
        "3. **Distribution Analysis**:\n",
        "    * Use a **Histogram** to see the spread of active days.\n",
        "    * Use a **CDF (Cumulative Distribution Function)** to identify the \"Heavy Users\".\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "MYWmPoAN1k_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_final['date'] = df_final['used_at'].dt.date\n",
        "\n",
        "# Calculate Active Days\n",
        "active_days = df_final.groupby('pid')['date'].nunique()\n",
        "\n",
        "# Calculate Daily Intensity (Hours per day per user)\n",
        "daily_usage = df_final.groupby(['pid', 'date'])['duration'].sum() / 3600\n",
        "avg_daily_hours = daily_usage.groupby('pid').mean()\n",
        "\n",
        "# 2. Visualizing User Distribution\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# A. Histogram: Active Days\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(active_days, bins=15, color='skyblue', kde=True)\n",
        "plt.title(\"User Persistence\\n(Active Days Distribution)\")\n",
        "plt.xlabel(\"Days Active\")\n",
        "\n",
        "# B. Histogram: Avg Daily Hours\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(avg_daily_hours, bins=20, color='salmon', kde=True)\n",
        "plt.title(\"Usage Intensity\\n(Avg Hours/Day)\")\n",
        "plt.xlabel(\"Hours\")\n",
        "\n",
        "# C. CDF Plot\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.ecdfplot(avg_daily_hours, color='green', lw=2)\n",
        "plt.axhline(0.8, color='grey', linestyle='--', alpha=0.7, label='80%')\n",
        "plt.axvline(avg_daily_hours.quantile(0.8), color='orange', linestyle='--')\n",
        "plt.title(\"Intensity CDF\")\n",
        "plt.xlabel(\"Avg Hours/Day\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Final Summary Stats\n",
        "print(f\"Summary Statistics:\")\n",
        "print(f\"- Average Active Days: {active_days.mean():.1f} days\")\n",
        "print(f\"- Median Daily Usage: {avg_daily_hours.median():.2f} hours\")\n",
        "print(f\"- 80% of users spend less than {avg_daily_hours.quantile(0.8):.2f} hours per day.\")"
      ],
      "metadata": {
        "id": "HNhRutfV1sMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why these plots?\n",
        "1. Tells us how \"regular\" our users are. If most users only appear for 1-2 days, our conclusions about their \"habits\" might be weak. We want to see a spread that shows consistent engagement.\n",
        "2. Understanding the \"average\" and the \"extreme\".\n",
        "3. The CDF helps us answer: \"What defines a 'Heavy User'?\"\n",
        "    * For example, the 80% line allows us to see the threshold where the top 20% of users reside.\n",
        "\n",
        "4. Outlier Detection: If a user browses 20 hours a day, are they a human or a bot?\n"
      ],
      "metadata": {
        "id": "KNz31G70vB6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Feature Engineering (Temporal & Categorical)\n",
        "\n",
        "We need to transform low-level logs into high-level behavioral features. By breaking down timestamps and mapping URLs to categories, we can understand *when* people are active and *what* they are doing.\n",
        "\n",
        "#### Temporal Features\n",
        "* **Action**: Extract `hour` and `day_name` from `used_at`.\n",
        "\n",
        "\n",
        "#### Content Categorization\n",
        "* **Action**: Map each record to a category.\n",
        "\n",
        "> **Tip:**\n",
        ">We use a `left merge` to ensure every browsing record is kept.\n",
        "> Since both dataframes now share the same column name (`domain`), we can use `pd.merge(..., on='domain')`. Always check for `NaN` values in the new `category` column to ensure your mapping coverage is sufficient.\n",
        "> If a domain isn't in your map, use `.fillna('uncategorized')` to ensure your analysis remains complete.\n",
        "\n",
        "> **Reference:**\n",
        "> In professional settings, we could use services like [Webshrinker](https://webshrinker.com/) to automatically categorize millions of domains using machine learning and domain database lookups."
      ],
      "metadata": {
        "id": "kUHAvVPJvjuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEMPORAL FEATURE EXTRACTION ---\n",
        "# Extract hour and day name from the timestamp\n",
        "df_final['hour'] = df_final['used_at'].dt.hour\n",
        "df_final['day_name'] = df_final['used_at'].dt.day_name()\n",
        "\n",
        "# --- CATEGORY MAPPING ---\n",
        "# Perform a left join to enrich the browsing data with website categories\n",
        "# Both dataframes use 'domain' as the join key\n",
        "df_final = pd.merge(\n",
        "    df_final,\n",
        "    category_df,\n",
        "    on='domain',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill domains not found in the category map with 'uncategorized'\n",
        "df_final['category'] = df_final['category'].fillna('uncategorized')\n",
        "\n",
        "# --- DATA VALIDATION (Print Statistics) ---\n",
        "\n",
        "# Ensure no users (pids) were lost during the merge\n",
        "unique_pids = df_final['pid'].nunique()\n",
        "print(f\"Total Unique Users (pid) retained: {unique_pids}\")\n",
        "print(f\"Total Records in table: {len(df_final)}\")\n",
        "\n",
        "# Preview the enriched table\n",
        "print(\"\\n--- Feature Table Preview ---\")\n",
        "# Focus on the relationship between pid, time, and category\n",
        "display(df_final[['pid', 'used_at', 'hour', 'day_name', 'domain', 'category']].head())\n",
        "\n",
        "# Categorization Summary\n",
        "print(\"\\n--- Category Coverage Summary ---\")\n",
        "cat_counts = df_final['category'].value_counts()\n",
        "print(cat_counts)\n",
        "\n",
        "# Verification Visualization: Quick check of data distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "cat_counts.head(10).sort_values().plot(kind='barh', color='skyblue')\n",
        "plt.title(\"Distribution of Top 10 Categories\")\n",
        "plt.xlabel(\"Number of Records\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zt7s06_OvnXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Behavioral Visualization\n",
        "\n",
        "Each user has a unique \"browsing profile\"â€” an image of when and how they interact with the web. By visualizing the **7x24 hour rhythm** of a specific user, we can uncover their personal habits.\n",
        "\n",
        "#### The Weekly Rhythm (Heatmap)\n",
        "We will create a heatmap that shows activity intensity across different hours and days for a **single user**.\n",
        "\n",
        "> **Tip:**\n",
        "> A \"Global Heatmap\" (all users) often washes out individual traits. Filtering for a specific `pid` allows us to see true human behaviorâ€”like consistent late-night browsing on weekends vs. structured work hours on weekdays.\n",
        "\n",
        "\n",
        "We will define a function `plot_weekly_heatmap` to make this visualization reusable for any user in the dataset."
      ],
      "metadata": {
        "id": "q7xlwEZUv-X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weekly_heatmap(df, user_id):\n",
        "    \"\"\"\n",
        "    Generates a 7x24 heatmap showing browsing density for a specific user.\n",
        "    \"\"\"\n",
        "    # Filter data for the specific user\n",
        "    user_df = df[df['pid'] == user_id].copy()\n",
        "\n",
        "    # 2Define the logical order of days for the Y-axis\n",
        "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    hours_order = list(range(24))\n",
        "\n",
        "    # Index: Days of the week, Columns: Hours of the day\n",
        "    # We count the number of sessions (using 'pid' as a proxy for count)\n",
        "    pivot_table = user_df.pivot_table(\n",
        "        index='day_name',\n",
        "        columns='hour',\n",
        "        values='pid',\n",
        "        aggfunc='count'\n",
        "    ).reindex(days_order)\n",
        "\n",
        "    # Fill hours with no activity with 0\n",
        "    pivot_table = pivot_table.fillna(0)\n",
        "    pivot_table = pivot_table.reindex(index=days_order, columns=hours_order, fill_value=0)\n",
        "    # 5. å¯è§†åŒ–\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(24, 6))\n",
        "    sns.heatmap(\n",
        "        pivot_table,\n",
        "        cmap=\"YlGnBu\",\n",
        "        annot=False,\n",
        "        cbar_kws={'label': 'Activity Intensity (Session Count)'}\n",
        "    )\n",
        "\n",
        "    plt.title(f\"Weekly Browsing Density Heatmap: User {user_id}\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Hour of Day (0-23)\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sYgQAYtc13dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CALL THE FUNCTION ---\n",
        "\n",
        "# Find the most active user to ensure we have an interesting result\n",
        "top_user_id = df_final['pid'].value_counts().idxmax()\n",
        "print(f\"Analyzing Digital Fingerprint for Top User: {top_user_id}\")\n",
        "\n",
        "# Run the function\n",
        "plot_weekly_heatmap(df_final, top_user_id)"
      ],
      "metadata": {
        "id": "guR9Ob_c1682"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top Websites (Bar Charts)\n",
        "Now that we know *when* this user is active, let's see *where* they go. We will identify the most frequently visited domains for the same user.\n",
        "\n",
        "#### Online Activities Overview (Bubble Chart)\n",
        "To get a high-level view of this user's interests, we will use a **Bubble Chart**. Unlike a standard bar chart, a bubble chart represents the frequency of visits per category through the area of a circle, providing a more intuitive sense of \"interest density.\"\n",
        "\n",
        "> **Insight:**\n",
        "> Comparing specific domains with general categories helps us distinguish between a user's **tools** (e.g., Google, GitHub) and their **hobbies** (e.g., Entertainment, Social Media)."
      ],
      "metadata": {
        "id": "PfO2vaPJwLOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import circlify\n",
        "\n",
        "def plot_top_websites(df, user_id, top_n=10):\n",
        "    \"\"\"\n",
        "    Plots a horizontal bar chart of the top N most visited domains for a specific user.\n",
        "    \"\"\"\n",
        "    user_df = df[df['pid'] == user_id].copy()\n",
        "    top_sites = user_df['domain'].value_counts().head(top_n)\n",
        "\n",
        "    if top_sites.empty:\n",
        "        print(f\"No data found for User: {user_id}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    colors = plt.cm.Blues(np.linspace(0.4, 0.9, top_n))\n",
        "    plt.barh(top_sites.index, top_sites.values, color=colors)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(f\"Top {top_n} Most Visited Websites (User: {user_id})\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"Visit Count\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_activity_bubbles(df, user_id):\n",
        "    \"\"\"\n",
        "    Generates a bubble chart of categorized online activities for a specific user.\n",
        "    \"\"\"\n",
        "    user_df = df[df['pid'] == user_id].copy()\n",
        "\n",
        "    # Aggregate counts by category, excluding noise\n",
        "    agg = (\n",
        "        user_df[~user_df['category'].isin(['uncategorized', 'other'])]\n",
        "        .groupby('category')['pid']\n",
        "        .count()\n",
        "        .reset_index(name='count')\n",
        "    )\n",
        "\n",
        "    if not agg.empty:\n",
        "        # Compute circle positions using circlify\n",
        "        circles = circlify.circlify(\n",
        "            agg['count'].tolist(),\n",
        "            show_enclosure=False,\n",
        "            target_enclosure=circlify.Circle(0, 0, 1.0)\n",
        "        )\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_aspect(\"equal\")\n",
        "\n",
        "        # Dynamically set limits to prevent bubbles from being cut off\n",
        "        max_r = 0\n",
        "        for circle in circles:\n",
        "            dist = np.sqrt(circle.x**2 + circle.y**2) + circle.r\n",
        "            if dist > max_r:\n",
        "                max_r = dist\n",
        "\n",
        "        ax.set_xlim(-max_r * 1.05, max_r * 1.05)\n",
        "        ax.set_ylim(-max_r * 1.05, max_r * 1.05)\n",
        "\n",
        "        cmap = plt.get_cmap(\"tab20\")\n",
        "\n",
        "        for circle, label, i in zip(circles, agg['category'], range(len(agg))):\n",
        "            x, y, r = circle.x, circle.y, circle.r\n",
        "            ax.add_patch(plt.Circle((x, y), r, facecolor=cmap(i % 20), alpha=0.8, edgecolor=\"white\", linewidth=2))\n",
        "\n",
        "            if r > 0.05:\n",
        "                ax.text(x, y, label, ha=\"center\", va=\"center\",\n",
        "                        fontsize=max(8, int(r*80)), fontweight=\"bold\")\n",
        "\n",
        "        plt.title(f\"Online Activity Overview: User {user_id}\", fontsize=14, fontweight='bold')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No categorized data found for User: {user_id}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TMp0oSJh2d-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CALL THE FUNCTIONS FOR DEMO ---\n",
        "\n",
        "target_pid = df_final['pid'].value_counts().idxmax()\n",
        "print(f\"Generating content profile for User: {target_pid}\")\n",
        "\n",
        "plot_top_websites(df_final, target_pid)\n",
        "plot_activity_bubbles(df_final, target_pid)"
      ],
      "metadata": {
        "id": "AzF48Ft_2hp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Search Intent (Word Cloud)\n",
        "While categories tell us *what* type of site a user visits, **Search Queries** tell us *exactly* what they are looking for. By extracting query parameters from search engine URLs (like Google or Bing), we can visualize the user's specific interests.\n",
        "\n",
        "> **Insight:**\n",
        "> This visualization completes the \"Digital Profile.\" We now know **when** they browse (Heatmap), **where** they go (Top Domains), **what** they do (Bubble Chart), and finally, **what they think about** (Word Cloud).\n",
        "\n",
        "**Note**: Since the dataset originates from Germany, we will filter out both English and German \"Stop Words\" (common words like 'the', 'und', 'der') to ensure only meaningful terms appear."
      ],
      "metadata": {
        "id": "0WKkuXCYwSgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_user_wordcloud(df, user_id):\n",
        "    # Combine English and German stop words to filter out noise\n",
        "    stop_words = set(get_stop_words(\"english\")) | set(get_stop_words(\"german\"))\n",
        "    # Add common search-related noise words manually if needed\n",
        "    # stop_words.update(['google', 'search', 'html', 'index', 'php'])\n",
        "\n",
        "    # Helper function to extract search terms from URL query parameters\n",
        "    def extract_search_query(url):\n",
        "        # Look for 'q=' or 'query=' in the URL string\n",
        "        match = re.search(r'[?&](q|query)=([^&]+)', str(url))\n",
        "        if match:\n",
        "            # Decode URL characters (e.g., %20 to space) and replace '+' with space\n",
        "            query = unquote(match.group(2)).replace('+', ' ')\n",
        "            return query\n",
        "        return \"\"\n",
        "\n",
        "    # Filter data for the specific user and extract queries\n",
        "    user_data = df[df['pid'] == user_id].copy()\n",
        "    queries = user_data['url'].apply(extract_search_query)\n",
        "\n",
        "    # Combine all queries into a single string\n",
        "    all_text = \" \".join(queries).lower()\n",
        "\n",
        "    # Tokenize words: Keep only alphanumeric characters (including German umlauts)\n",
        "    # Using \\wÃ¤Ã¶Ã¼ÃŸ to support German specific characters\n",
        "    words = re.findall(r\"[\\wÃ¤Ã¶Ã¼ÃŸ]+\", all_text)\n",
        "\n",
        "    # Filter: Remove stop words and very short words (less than 3 chars)\n",
        "    filtered_words = [w for w in words if w not in stop_words and len(w) >= 3]\n",
        "\n",
        "    # --- STEP 2: GENERATION ---\n",
        "    if not filtered_words:\n",
        "        print(f\"No search queries found for User: {user_id}\")\n",
        "        return\n",
        "\n",
        "    # Count frequencies for better WordCloud control\n",
        "    word_freq = Counter(filtered_words)\n",
        "\n",
        "    # Generate the WordCloud object\n",
        "    # Plasma colormap and high resolution for a professional look\n",
        "    wc = WordCloud(\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"plasma\",\n",
        "        max_words=100\n",
        "    ).generate_from_frequencies(word_freq)\n",
        "\n",
        "    # --- STEP 3: VISUALIZATION ---\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\") # Hide axes for the cloud\n",
        "    plt.title(f\"Search Intent Word Cloud (User: {user_id})\", fontsize=20, fontweight='bold')\n",
        "    plt.show()\n",
        "# Execute the function for our target user\n",
        "plot_user_wordcloud(df_final, target_pid)\n"
      ],
      "metadata": {
        "id": "VMI3pzozwZHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Demographic Perspectives\n",
        "\n",
        "Data becomes truly powerful when we understand *who* is behind the screen. In this final section, we merge our behavioral features with sociodemographic data (**Age** and **Gender**) to identify group-level trends.\n",
        "\n",
        "#### Demographic Group Comparison\n",
        "We will use our **Engineered Features** (like Average Daily Usage and Categories) to answer:\n",
        "1.  Do different **Age Groups** have different browsing intensities?\n",
        "2.  How does **Gender** influence the types of content consumed?\n",
        "3. How does political orientation influence their news consumption?\n",
        "\n",
        "> **ðŸ’¡ Analytical Tip:**\n",
        "> Avoid using raw session counts for comparison, as one \"power user\" can skew the results. Instead, use **Averages per User** within each demographic group to ensure a fair comparison."
      ],
      "metadata": {
        "id": "OscdkPvizzNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We merge demo into our feature-rich 'df_final'\n",
        "df_demographic = pd.merge(df_final, demo_df, on='pid', how='left')\n",
        "\n",
        "# Calculate Average Daily Intensity per user first\n",
        "user_intensity = df_demographic.groupby(['pid', 'age_group', 'gender', 'date'])['duration'].sum() / 3600\n",
        "avg_user_intensity = user_intensity.groupby(['pid', 'age_group', 'gender']).mean().reset_index(name='avg_hours')\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Subplot 1: Age Group vs. Browsing Intensity\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(data=avg_user_intensity, x='age_group', y='avg_hours', hue='age_group', palette='Set2', legend=False)\n",
        "plt.title(\"Daily Browsing Intensity by Age Group\", fontsize=13, fontweight='bold')\n",
        "plt.ylabel(\"Avg Hours per Day\")\n",
        "plt.xlabel(\"Age Group\")\n",
        "\n",
        "# Subplot 2: Gender & Category Preferences\n",
        "# We look at the top 5 categories (excluding uncategorized) across genders\n",
        "top_cats = df_demographic[df_demographic['category'] != 'uncategorized']['category'].value_counts().head(5).index\n",
        "df_top_cats = df_demographic[df_demographic['category'].isin(top_cats)]\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "# Calculate percentage of sessions per category by gender\n",
        "cat_gender_dist = df_top_cats.groupby(['gender', 'category']).size().unstack(level=0)\n",
        "cat_gender_dist_pct = cat_gender_dist.div(cat_gender_dist.sum(axis=1), axis=0)\n",
        "\n",
        "cat_gender_dist_pct.plot(kind='barh', stacked=True, ax=plt.gca(), color=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Top 5 Categories: Gender Composition\", fontsize=13, fontweight='bold')\n",
        "plt.xlabel(\"Proportion of Sessions\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(title='Gender', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- STEP 4: FINAL SUMMARY ---\n",
        "overall_age_avg = avg_user_intensity.groupby('age_group')['avg_hours'].mean()\n",
        "print(\"--- Final Demographic Insights ---\")\n",
        "print(f\"Most active age group: {overall_age_avg.idxmax()} ({overall_age_avg.max():.2f} hrs/day)\")"
      ],
      "metadata": {
        "id": "HXoeeCmaz30i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge demographic data\n",
        "df_demographic = pd.merge(df_final, demo_df, on='pid', how='left')\n",
        "\n",
        "# 1 to Left (Blue) and 0 to Right (Red)\n",
        "df_demographic['politics_label'] = df_demographic['politics'].map({1: 'Left', 0: 'Right'})\n",
        "\n",
        "# Calculate Average Daily Intensity per user\n",
        "user_intensity = df_demographic.groupby(['pid', 'age_group', 'gender', 'politics_label', 'date'])['duration'].sum() / 3600\n",
        "avg_user_intensity = user_intensity.groupby(['pid', 'age_group', 'gender', 'politics_label']).mean().reset_index(name='avg_hours')\n",
        "\n",
        "# Create a large figure for the three-panel analysis\n",
        "plt.figure(figsize=(24, 7))\n",
        "\n",
        "# Subplot 1: Age Group vs. Browsing Intensity\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(data=avg_user_intensity, x='age_group', y='avg_hours', hue='age_group', palette='Set2', legend=False)\n",
        "plt.title(\"Daily Browsing Intensity by Age Group\", fontsize=13, fontweight='bold')\n",
        "plt.ylabel(\"Avg Hours per Day\")\n",
        "plt.xlabel(\"Age Group\")\n",
        "\n",
        "# Subplot 2: Gender & Category Preferences (Top 5)\n",
        "plt.subplot(1, 3, 2)\n",
        "top_cats = df_demographic[df_demographic['category'] != 'uncategorized']['category'].value_counts().head(5).index\n",
        "df_top_cats = df_demographic[df_demographic['category'].isin(top_cats)]\n",
        "\n",
        "cat_gender_dist = df_top_cats.groupby(['gender', 'category']).size().unstack(level=0)\n",
        "cat_gender_dist_pct = cat_gender_dist.div(cat_gender_dist.sum(axis=1), axis=0)\n",
        "\n",
        "cat_gender_dist_pct.plot(kind='barh', stacked=True, ax=plt.gca(), color=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Top 5 Categories: Gender Composition\", fontsize=13, fontweight='bold')\n",
        "plt.xlabel(\"Proportion of Sessions\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(title='Gender', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 3: Political Orientation & News Media Consumption\n",
        "plt.subplot(1, 3, 3)\n",
        "\n",
        "# Filter for news category and top 8 most visited domains\n",
        "df_news = df_demographic[df_demographic['category'] == 'news'].copy()\n",
        "top_news_domains = df_news['domain'].value_counts().head(8).index\n",
        "df_news_filtered = df_news[df_news['domain'].isin(top_news_domains)]\n",
        "\n",
        "# Calculate normalized proportions within each political group\n",
        "pol_domain_dist = df_news_filtered.groupby(['politics_label', 'domain']).size().reset_index(name='session_count')\n",
        "pol_domain_dist['proportion'] = pol_domain_dist.groupby('politics_label')['session_count'].transform(lambda x: x / x.sum())\n",
        "\n",
        "# Use specific colors: Blue for Left (1), Red for Right (0)\n",
        "custom_palette = {'Left': '#1f77b4', 'Right': '#d62728'}\n",
        "\n",
        "sns.barplot(\n",
        "    data=pol_domain_dist,\n",
        "    y='domain',\n",
        "    x='proportion',\n",
        "    hue='politics_label',\n",
        "    palette=custom_palette\n",
        ")\n",
        "\n",
        "\n",
        "plt.title(\"News Consumption by Political Orientation\", fontsize=13, fontweight='bold')\n",
        "plt.xlabel(\"Share of Group's Total News Sessions\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(title='Politics', loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- STEP 4: FINAL SUMMARY ---\n",
        "print(\"--- Final Demographic Insights ---\")\n",
        "overall_age_avg = avg_user_intensity.groupby('age_group')['avg_hours'].mean()\n",
        "print(f\"1. Most active age group: {overall_age_avg.idxmax()} ({overall_age_avg.max():.2f} hrs/day)\")\n",
        "\n",
        "print(\"\\n2. Political News Preference (Top 3 per Group):\")\n",
        "for pol in ['Left', 'Right']:\n",
        "    subset = df_news[df_news['politics_label'] == pol]\n",
        "    if not subset.empty:\n",
        "        top_3 = subset['domain'].value_counts().head(3).index.tolist()\n",
        "        print(f\"   - {pol}: {', '.join(top_3)}\")"
      ],
      "metadata": {
        "id": "H7Gh1mjrz6vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini Project\n",
        "\n",
        "Instead of looking at a single user, Please try to analyze the entire population of 50 users.\n",
        "\n",
        "### What you can do?\n",
        "1.  **Task 1: Define Your Own Tribe**:\n",
        "    * Look at the behavioral data and decide on a rule to split the 50 users into 2 or more groups.\n",
        "    * *Example inspiration*: Are they \"Late-Night Browsers\"? \"Social Media Addicts\"?\n",
        "    * **Visualization**: Use the `plot_segment_rhythms` or `plot_segment_categories` function to see if your tribes actually have different patterns.\n",
        "2.  **Task 2: Link with Demographics & Politics**\n",
        "Let's link behavior to **who they are**.\n",
        "    * Does demographic variables (Gender, Age, Political Orientation) actually result in different browsing habits?\n"
      ],
      "metadata": {
        "id": "V1i35ElW3ECn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_segment_rhythms(df, mapping_dict):\n",
        "    \"\"\"\n",
        "    Visualizes the average 24h rhythm for defined segments.\n",
        "    mapping_dict: {pid: 'Label'}\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "    df_temp['segment'] = df_temp['pid'].map(mapping_dict)\n",
        "\n",
        "\n",
        "    rhythm = df_temp.groupby(['segment', 'pid', 'hour']).size().unstack(fill_value=0)\n",
        "\n",
        "    rhythm_norm = rhythm.div(rhythm.max(axis=1), axis=0)\n",
        "    avg_rhythm = rhythm_norm.groupby('segment').mean()\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for label in avg_rhythm.index:\n",
        "        plt.plot(avg_rhythm.columns, avg_rhythm.loc[label], label=label, marker='o', lw=3)\n",
        "\n",
        "    plt.title(\"24-Hour Rhythm Comparison Between Your Tribes\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Hour of Day\")\n",
        "    plt.ylabel(\"Relative Activity Level\")\n",
        "    plt.xticks(range(24))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U2C1ax0A9yf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_segment_categories(df, mapping_dict, target_categories=None):\n",
        "    \"\"\"\n",
        "    Visualizes actual time distribution. Categories not in target_categories\n",
        "    are grouped into 'Others' to keep the 100% scale realistic.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "\n",
        "    df_temp['segment'] = df_temp['pid'].map(mapping_dict)\n",
        "    df_temp = df_temp[df_temp['segment'].notna()]\n",
        "\n",
        "\n",
        "    if target_categories:\n",
        "        df_temp['category_clean'] = df_temp['category'].apply(\n",
        "            lambda x: x if x in target_categories else 'Others'\n",
        "        )\n",
        "    else:\n",
        "        df_temp['category_clean'] = df_temp['category']\n",
        "\n",
        "\n",
        "    cat_stats = df_temp.groupby(['segment', 'category_clean'])['duration'].sum().unstack(fill_value=0)\n",
        "    cat_stats_pct = cat_stats.div(cat_stats.sum(axis=1), axis=0) * 100\n",
        "\n",
        "\n",
        "\n",
        "    if 'Others' in cat_stats_pct.columns:\n",
        "        cols = [c for c in cat_stats_pct.columns if c != 'Others'] + ['Others']\n",
        "        cat_stats_pct = cat_stats_pct[cols]\n",
        "\n",
        "    ax = cat_stats_pct.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='Paired')\n",
        "\n",
        "    plt.title(\"Actual Content DNA: Share of Total Time per Tribe\", fontsize=15, fontweight='bold')\n",
        "    plt.ylabel(\"Percentage of Actual Total Time (%)\")\n",
        "    plt.xlabel(\"Tribe (Grouped by Top Interest)\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(title=\"Web Categories\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "\n",
        "    for p in ax.patches:\n",
        "        h = p.get_height()\n",
        "        if h > 5:\n",
        "            ax.text(p.get_x() + p.get_width()/2, p.get_y() + h/2, f'{h:.1f}%',\n",
        "                    ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "SDc79PeG6gJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- WORK AREA----------\n",
        "# Define your group logic\n",
        "# Create the mapping {pid: 'Label'}\n"
      ],
      "metadata": {
        "id": "MafMlRRj1xuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_demographic['used_at'] = pd.to_datetime(df_demographic['used_at'])\n",
        "df_demographic['hour'] = df_demographic['used_at'].dt.hour\n",
        "\n",
        "# Define 'Early Birds' vs 'Night Owls'\n",
        "def categorize_user(user_data):\n",
        "    avg_hour = user_data['hour'].mean()\n",
        "    # If their average activity is before 2 PM, call them Early Birds\n",
        "    if avg_hour < 14:\n",
        "        return 'Early Birds'\n",
        "    else:\n",
        "        return 'Night Owls'\n",
        "\n",
        "# 3. Create the mapping {pid: 'Label'}\n",
        "\n",
        "mapping = df_demographic.groupby('pid').apply(categorize_user).to_dict()\n",
        "\n",
        "# 5. RUN THE VISUALIZATION\n",
        "plot_segment_rhythms(df_demographic, mapping)"
      ],
      "metadata": {
        "id": "aKA8TV5C1wn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 3 Tribes\n",
        "target_tribe_names = ['social_media', 'news', 'entertainment']\n",
        "\n",
        "# Assign every user to their top category\n",
        "user_main_feat = df_final.groupby(['pid', 'category'])['duration'].sum().unstack(fill_value=0).idxmax(axis=1)\n",
        "\n",
        "# Create the mapping ONLY for those 3 categories\n",
        "\n",
        "mapping_feat = {\n",
        "    pid: f\"{feat.title()} Tribe\"\n",
        "    for pid, feat in user_main_feat.items()\n",
        "    if feat in target_tribe_names\n",
        "}\n",
        "\n",
        "# target_categories get their own color.\n",
        "# Everything else (search, shopping, etc.) goes into 'Others'.\n",
        "plot_segment_categories(\n",
        "    df_final,\n",
        "    mapping_feat,\n",
        "    target_categories=['social_media', 'news', 'entertainment']\n",
        ")"
      ],
      "metadata": {
        "id": "jWfEHEpl4ria"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Own Digital Profile\n",
        "\n",
        "Do you want to see your own \"Browsing profile\"? You can apply today's entire pipeline to your personal browsing history!\n",
        "\n",
        "#### How to do it at home:\n",
        "1. **Export your history**:\n",
        "   * Use the Chrome extension ([Export Chrome History](https://chromewebstore.google.com/detail/export-chrome-history/dihloblpkeiddiaojbagoecedbfpifdj?hl=en)).\n",
        "   * It will generate a CSV file containing columns like `date`, `time`, `url`, and `visitCount`.\n",
        "2. **Align the data**:\n",
        "   * Upload your CSV to this notebook.\n",
        "3. **Run the visualizations**:\n",
        "   * Re-run these functions using your data.\n",
        "\n",
        "\n",
        "(if you want, share it tomorrow)\n",
        "\n",
        "\n",
        "### Pre-reading & Resources on LMM\n",
        "If you need a refresher on Linear Mixed Models, please refer to:\n",
        "* **The \"Plain English\" Guide**: [Introduction to Mixed Effects Models](https://ourcodingclub.github.io/tutorials/mixed-models/) (Coding Club)"
      ],
      "metadata": {
        "id": "6vdB9ja6z8W7"
      }
    }
  ]
}